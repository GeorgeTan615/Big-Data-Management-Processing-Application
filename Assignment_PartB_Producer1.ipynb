{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT3182 Major Assignment Part B Task 1a (Event Producer 1)\n",
    "## George Tan Juan Sheng (30884128)\n",
    "### Part B Task 1a\n",
    "#### Write a python program that loads all the data from climate_streaming.csv and randomly (with replacement) feed the data to the stream every 10 seconds. You will need to append additional information such as producer information to identify the producer and created date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to connect to our MongoClient and access the collection we have made from Part A (so that we can get the latest date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "client = MongoClient () \n",
    "db = client.fit3182_assignment_db\n",
    "collection = db.partA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we would have to write our program so that we would be able to feed our data to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Message published successfully. Data: {'latitude': -37.641999999999996, 'longitude': 149.263, 'air_temperature_celcius': 20, 'relative_humidity': 55.8, 'windspeed_knots': 10.5, 'max_wind_speed': 15.9, 'precipitation': 0.01, 'precipitation_flag': 'G', 'GHI_w/m2': 163, 'producer': 'climate_streaming', 'created_date': '01/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.779, 'longitude': 146.108, 'air_temperature_celcius': 13, 'relative_humidity': 42.0, 'windspeed_knots': 11.4, 'max_wind_speed': 16.9, 'precipitation': 0.0, 'precipitation_flag': 'G', 'GHI_w/m2': 119, 'producer': 'climate_streaming', 'created_date': '02/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.375, 'longitude': 148.063, 'air_temperature_celcius': 10, 'relative_humidity': 43.5, 'windspeed_knots': 12.0, 'max_wind_speed': 16.9, 'precipitation': 0.04, 'precipitation_flag': 'G', 'GHI_w/m2': 90, 'producer': 'climate_streaming', 'created_date': '03/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.0005, 'longitude': 143.1847, 'air_temperature_celcius': 17, 'relative_humidity': 52.5, 'windspeed_knots': 7.7, 'max_wind_speed': 16.9, 'precipitation': 0.0, 'precipitation_flag': 'I', 'GHI_w/m2': 143, 'producer': 'climate_streaming', 'created_date': '04/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.0884, 'longitude': 141.0357, 'air_temperature_celcius': 12, 'relative_humidity': 40.4, 'windspeed_knots': 5.9, 'max_wind_speed': 8.9, 'precipitation': 0.0, 'precipitation_flag': 'G', 'GHI_w/m2': 111, 'producer': 'climate_streaming', 'created_date': '05/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.633, 'longitude': 149.264, 'air_temperature_celcius': 16, 'relative_humidity': 50.9, 'windspeed_knots': 12.9, 'max_wind_speed': 21.0, 'precipitation': 0.03, 'precipitation_flag': 'G', 'GHI_w/m2': 136, 'producer': 'climate_streaming', 'created_date': '06/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.3823, 'longitude': 141.3146, 'air_temperature_celcius': 10, 'relative_humidity': 43.7, 'windspeed_knots': 9.5, 'max_wind_speed': 13.0, 'precipitation': 0.08, 'precipitation_flag': 'G', 'GHI_w/m2': 90, 'producer': 'climate_streaming', 'created_date': '07/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.8147, 'longitude': 143.1062, 'air_temperature_celcius': 17, 'relative_humidity': 46.4, 'windspeed_knots': 9.5, 'max_wind_speed': 20.0, 'precipitation': 0.0, 'precipitation_flag': 'I', 'GHI_w/m2': 150, 'producer': 'climate_streaming', 'created_date': '08/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.5915, 'longitude': 143.0015, 'air_temperature_celcius': 17, 'relative_humidity': 52.0, 'windspeed_knots': 8.8, 'max_wind_speed': 15.0, 'precipitation': 0.0, 'precipitation_flag': 'I', 'GHI_w/m2': 143, 'producer': 'climate_streaming', 'created_date': '09/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -37.602, 'longitude': 149.295, 'air_temperature_celcius': 17, 'relative_humidity': 50.8, 'windspeed_knots': 9.0, 'max_wind_speed': 15.9, 'precipitation': 0.0, 'precipitation_flag': 'G', 'GHI_w/m2': 145, 'producer': 'climate_streaming', 'created_date': '10/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.7685, 'longitude': 142.7134, 'air_temperature_celcius': 14, 'relative_humidity': 48.2, 'windspeed_knots': 12.5, 'max_wind_speed': 19.0, 'precipitation': 0.03, 'precipitation_flag': 'G', 'GHI_w/m2': 122, 'producer': 'climate_streaming', 'created_date': '11/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.3114, 'longitude': 142.7605, 'air_temperature_celcius': 28, 'relative_humidity': 56.7, 'windspeed_knots': 9.3, 'max_wind_speed': 16.9, 'precipitation': 0.0, 'precipitation_flag': 'I', 'GHI_w/m2': 227, 'producer': 'climate_streaming', 'created_date': '12/01/2022'}\n",
      "Message published successfully. Data: {'latitude': -36.7801, 'longitude': 142.7151, 'air_temperature_celcius': 16, 'relative_humidity': 51.7, 'windspeed_knots': 7.4, 'max_wind_speed': 15.0, 'precipitation': 0.0, 'precipitation_flag': 'I', 'GHI_w/m2': 135, 'producer': 'climate_streaming', 'created_date': '13/01/2022'}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "\n",
    "# Reads data from climate_streaming.csv, puts each row data into a document and appends all such documents into a list.\n",
    "def read_climate_streaming():\n",
    "    climate_streaming = pd.read_csv('climate_streaming.csv')\n",
    "    climate_streaming.rename(columns = {'precipitation ':'precipitation'}, inplace = True)\n",
    "    climate_streaming['precipitation_flag'] = climate_streaming['precipitation'].str[-1]\n",
    "    climate_streaming['precipitation'] = climate_streaming['precipitation'].str[0:-1]\n",
    "    \n",
    "    data = []\n",
    "    for index,climateRow in climate_streaming.iterrows():\n",
    "        document = {}\n",
    "        document['latitude'] = float(climateRow['latitude'])\n",
    "        document['longitude'] = float(climateRow['longitude'])\n",
    "        document['air_temperature_celcius'] = int(climateRow['air_temperature_celcius'])\n",
    "        document['relative_humidity'] = float(climateRow['relative_humidity'])\n",
    "        document['windspeed_knots'] = float(climateRow['windspeed_knots'])\n",
    "        document['max_wind_speed'] = float(climateRow['max_wind_speed'])\n",
    "        document['precipitation'] = float(climateRow['precipitation'].strip()) #Remove any leading and trailing spaces for precipitation data\n",
    "        document['precipitation_flag'] = climateRow['precipitation_flag'].strip()\n",
    "        document['GHI_w/m2'] = int(climateRow['GHI_w/m2'])\n",
    "        data.append(document)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Gets the latest date in our collection\n",
    "def get_latest_date():\n",
    "    latest_date = collection.aggregate([\n",
    "                {\"$sort\":{\"date\":-1}},\n",
    "                {\"$project\":{\"_id\":0,\"date\":1}},\n",
    "                {\"$limit\":1}\n",
    "                ])\n",
    "    for document in latest_date:\n",
    "        latest_date = document['date']\n",
    "    return latest_date\n",
    "    \n",
    "\n",
    "# Publishes message to Kafka\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, value=data)\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'PartB'\n",
    "    print('Publishing records..')\n",
    "    producer01 = connect_kafka_producer()\n",
    "    data = read_climate_streaming() # Gets all the documents produced from climate_streaming.csv\n",
    "    latest_date = get_latest_date() + timedelta(days=1) # After getting latest date, we would add one day to it to get the first date we would use to start feeding data\n",
    "    daysPassed = 0  # Tracks how many days we should add to our latest date \n",
    "\n",
    "    while True:\n",
    "        chosenData = random.choice(data) # Randomly chooses a document from our list of documents\n",
    "        curr_date = latest_date + timedelta(days=daysPassed) # Creates the date we will use to feed our data by adding the number of days passed to our latest date from Part A\n",
    "        chosenData['producer'] = \"climate_streaming\" \n",
    "        chosenData[\"created_date\"] = curr_date.strftime(\"%d/%m/%Y\")\n",
    "        publish_message(producer01, topic, chosenData)\n",
    "        daysPassed += 1 # After we insert a climate streaming data, add 1 day to daysPassed so the next date we would use to feed our data would be incremented by 1 day\n",
    "        sleep(10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
