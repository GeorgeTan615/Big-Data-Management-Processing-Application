{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT3182 Major Assignment Part B Task 1d (Streaming Application)\n",
    "## George Tan Juan Sheng (30884128)\n",
    "### Part B Task 1d\n",
    "#### Write a streaming application using the Apache Spark Structured Streaming API which processes data in batches of 10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make the necessary import statements as well as install pygeohash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygeohash in /home/student/.local/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import statements \n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at, when\n",
    "import pygeohash as pgh\n",
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# install pygeohash\n",
    "!pip install pygeohash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also need to connect to MongoClient and access our database's collection. We would also need to set our topic name so that we could retrieve the data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f9e416ee180>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our topic name\n",
    "topic_name = 'PartB'\n",
    "\n",
    "# Connect to MongoClient and access our collection\n",
    "client = MongoClient () \n",
    "db = client.fit3182_assignment_db\n",
    "collection = db.partB\n",
    "collection.delete_many({}) # Clear all previously added documemnts into this collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\n",
    "    .option('subscribe', topic_name)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "output_stream_df = (\n",
    "    topic_stream_df\n",
    "    .select(                                      \n",
    "        topic_stream_df.value\n",
    "        .alias('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we would need to define all of the processing of the batch of data we have received so that we could insert the document in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df,batch_id):\n",
    "    raw_data = batch_df.collect()\n",
    "    streams = [json.loads(item.asDict()['data']) for item in raw_data] # For each item, upon accessing its data, we need to convert the data from binary to json \n",
    "    hashMap = {}\n",
    "    climateData = None\n",
    "    \n",
    "    # Group streams based on location (precision 3)\n",
    "    for stream in streams:\n",
    "        # Find our stream that is our climate data (from climate_streaming producer)\n",
    "        if stream['producer'] == 'climate_streaming':\n",
    "            climateData = stream\n",
    "        \n",
    "        # For each stream, we would hash the stream into the hashMap based on its location\n",
    "        location = pgh.encode(stream['latitude'],stream['longitude'],precision=3)\n",
    "        if location in hashMap:\n",
    "            hashMap[location].append(stream)\n",
    "        else:\n",
    "            hashMap[location] = [stream]\n",
    "    \n",
    "    # If we found a climate data in the streams\n",
    "    if climateData is not None:\n",
    "        # Convert date to datetime object\n",
    "        climateData['date'] = datetime.strptime(climateData['created_date'],'%d/%m/%Y') \n",
    "        del climateData['created_date'] # Rename column to match part A, remove previous column\n",
    "        \n",
    "        # Get the location of the climate data\n",
    "        climateLocation = pgh.encode(climateData['latitude'],climateData['longitude'],precision=3)\n",
    "        \n",
    "        # If we have one or more hotspot data in that climate location\n",
    "        if len(hashMap[climateLocation]) > 1:\n",
    "            hotspots = {}\n",
    "            \n",
    "            # Group hotspot according to location (precision 5)\n",
    "            for stream in hashMap[climateLocation]:\n",
    "                producer = stream['producer']\n",
    "                \n",
    "                # If the stream's producer is not climate_streaming, means it is a hotspot data\n",
    "                if producer != 'climate_streaming':\n",
    "                    # Hash the hotspots based on hotspot's precision 5 location\n",
    "                    location = pgh.encode(stream['latitude'],stream['longitude'],precision=5) \n",
    "                    if location in hotspots:\n",
    "                        hotspots[location].append(stream)\n",
    "                    else:\n",
    "                        hotspots[location] = [stream]\n",
    "                        \n",
    "            # Calculate fireCause according to climate data\n",
    "            if climateData['air_temperature_celcius']> 20 and climateData['GHI_w/m2']>180:\n",
    "                fireCause = 'natural'\n",
    "            else:\n",
    "                fireCause = 'other'\n",
    "            \n",
    "            # Keep track of the documents needed to be added to climate data's hotspots\n",
    "            hotspotDocs = []\n",
    "            \n",
    "            # Loop through each precision 5 location in our hotspots dictionary\n",
    "            for location in hotspots.keys():\n",
    "                \n",
    "                hotspotDoc = None # Keep track of the document we would use to insert if we have one or more hotspots in a location\n",
    "                totalSurfaceTemp = 0 # Keep track of the total surface temperature for all the hotspots in a location. Would be useful for calculating average surface temperature for more than two hotspots in the same location later\n",
    "                totalConfidence = 0 # Keep track of the total confidence for all the hotspots in a location. Would be useful for calculating average confidence for more than two hotspots in the same location later\n",
    "                numberOfData = 0 # Keep track of the number of hotspots in one location. Would be useful for calculating average confidence and surface temperature for more than two hotspots in the same location later\n",
    "                \n",
    "                # Loop through each hotspot(fireOccurence) in each location\n",
    "                for fire in hotspots[location]:\n",
    "                    # Update the variables we have set earlier\n",
    "                    hotspotDoc = fire \n",
    "                    totalSurfaceTemp += fire['surface_temperature_celcius']\n",
    "                    totalConfidence += fire['confidence']\n",
    "                    numberOfData += 1\n",
    "                \n",
    "                avgSurfaceTemp = int(totalSurfaceTemp/numberOfData) # Calculate average surface temperature. If we had only one hotspot, the result would be unaffected\n",
    "                avgConfidence = int(totalConfidence/numberOfData) # Calculate average confidence. If we had only one hotspot, the result would be unaffected\n",
    "                hotspotDoc['surface_temperature_celcius'] = avgSurfaceTemp  # Update the surface temperature celcius in the hotspot document we chosen\n",
    "                hotspotDoc['confidence'] = avgConfidence # Update the confidence in the hotspot document we chosen\n",
    "                \n",
    "                # To handle misaligned dates, make sure hotspot dates match the climate data's date\n",
    "                curr_datetime = datetime.strptime(hotspotDoc['created_datetime'],'%d/%m/%Y %H:%M:%S')\n",
    "                new_datetime = climateData['date'].replace(hour = curr_datetime.hour, minute = curr_datetime.minute)\n",
    "                hotspotDoc['datetime'] = new_datetime\n",
    "                del hotspotDoc['created_datetime'] # Rename column to match part A, remove previous column\n",
    "                \n",
    "                hotspotDoc['cause'] = fireCause # Insert the fire cause\n",
    "                del hotspotDoc['producer'] # Remove producer from hotspot document\n",
    "                hotspotDocs.append(hotspotDoc) # Add this hotspot document into the list of hotspots which will be inserted into the climate data later on\n",
    "\n",
    "            climateData['hotspots'] = hotspotDocs\n",
    "\n",
    "    # If we found a climate data earlier, we will insert the climateData that we have prepared, else just don't insert anything\n",
    "    if climateData is not None:\n",
    "        del climateData['producer'] # Remove producer from climate document\n",
    "        del climateData['latitude'] # Remove latitude and longitud e\n",
    "        del climateData['longitude']\n",
    "        climateData['station'] = 948701 # Give station a constant value\n",
    "        collection.insert_one(climateData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put a trigger for 10 seconds so we only collect the data for every 10 seconds\n",
    "# foreachBatch is used instead so that we are able to process all the data in a batch at once\n",
    "db_writer = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime = '10 seconds') \n",
    "    .foreachBatch(process_batch) \n",
    ")\n",
    "\n",
    "console_logger = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = db_writer\n",
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
